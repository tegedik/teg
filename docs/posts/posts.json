[
  {
    "path": "posts/aart/",
    "title": "Achen ve Üç Kuralı",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2020-06-15",
    "categories": [],
    "contents": "\nYazıya başlamadan önce başlığın Cristopher Achen’ın üç kuralı anlamına gelmediğini, Achen’ın “Toward A New Political Methodology: Microfoundations and ART.” (2002) isimli makalesinde bahsettiği Üç Kuralı’na (A Rule of Three) gönderme yaptığını belirtmeliyim. Yazıda bu makale ve Üç Kuralı üzerinde duracağım.1 Aslında Üç Kuralına makalenin sonlarında kısaca ve bir öneri olarak değiniliyor. Bu öneriye kadar Achen’ın yaptığı tartışmalar da ilginç. Kısaca bu bölümlere değinmek konuyla ilgili daha iyi bir fikir verebilir.\nGirişi takip eden üç bölümde Achen bilinen tahminleyicilerin (estimator) genelleştirilmesi konusunu tartışıyor. Özellikle logit’in iki genelleştirmesi üzerinde duruyor: scobit (skewed logit) ve power logit. Nicel analizlere aşina olmayanlar için bu bölümler biraz ürkütücü gelebilir, çok sayıda yunan harfi ve formül havada uçuşuyor. Aslında arkadaki fikir oldukça basit. Mesela iki kategorili (“başarı” ve “başarısızlık”) bir değişken için bir logit modeli düşünelim. Eğer \\(P\\) bu model altında başarılı olma olasılığı ise,\n\\[P = \\frac{1}{1 + e^{-z}}\\] bize lojistik dağılımın kümülafif dağılım fonksiyonunu (cumulatif distribution function - cdf) verecektir. Genelde model bir link fonksiyonuyla tamamlanır yani \\(z\\) bağımsız değişkenlerin lineer fonksiyonu olarak ifade edilir (\\(z_i = X_i\\beta\\)). Achen, scobit ve power logit’in bu modelin bir genelleştirmesi olarak düşünülebileceğini söylüyor. Mesela,\n\\[P_{i}^{**} = P_{i}^{a} \\frac{1}{(1 + e^{-X_i\\beta})^\\alpha}\\] bize power logit cdf’sini verir. Achen neden böyle bir genelleştirmeye ihtiyaç duyabileceğimizi, logit modelini random utility model olarak ifade ettikten sonra tartışıyor.2 Ancak burada daha fazla ayrıntıya girip açıklamamıza gerek yok. Zaten Achen’ın amacı da bu tahminleyicileri okura tanıtmak değil. Bundan ziyade sayıları gittikçe artan tahminleyicilerin gerekliliğini sorgulamak. Achen’a göre tahminleyicileri çoğaltmak sorun değil hatta tartışma sırasında iki tane (sahte) tahminleyici de kendisi uyduruyor (mixit ve clumpit). Sorun bu tahminleyicilerin istatiksel kesinliği (precision) düşürmesi. Mesela scobit, logit’e tek parametre ekliyor (\\(\\alpha\\)) ancak çok daha büyük örneklem gerektiriyor (Achen’ın örnek verdiği çalışmada 100.000’in üzerinde gözlem var). Achen, sadece eldeki veriye özgü bir durumu hesaba kattığı ya da daha genel olduğu için yeni bir tahminleyici kullanmayı sorunlu buluyor. Ayrıca sürekli daha genel tahminleyiciler üretmeye çalışmanın metodolojistlerin işi olmadığını söylüyor:\n\nStatisticians do that for a living, and we will never be as good at their job as they are. Trying to keep up will leave us forever second-rate —at best— and, more importantly, irrelevant to genuine empirical advance in the discipline. (Achen 2002, 437)\n\nMakalenin bundan sonra ampirik ilerlemeyi mümkün kılacak iki güzergaha işaret ettiğini söyleyebiliriz. İlkinde Achen, tahminleyicileri seçmek ve yapılandırmak için (siyasal) aktörlerin davranışlarına dair formel modellere başvurulması gerektiğini söylüyor. Bu formel modelin hangi teoriden geldiği önemli değil. Önemli olan formel modelin, istatistiksel tanımlamla (specification) için mikrotemel olması. Daha kaba bir ifadeyle, istatiksel model, bu formel modelden çıkmalı. Buna göre bir tahminleyici formel modele uygunsa kullanılmalı yoksa sırf yeni, moda ya da karmaşık olduğu için tercih edilmemeli. Achen’ın ifade şekli kimilerine yabancı gelebilir. Ancak arkasındaki fikir oldukça tanıdık: teori (ve teoriden çıkan model) analizi yönlendirmeli.\nİkinci güzergah ise daha zorlu çünkü ilkinin aksine elimizde formel bir model yok. Achen çoğu durumda araştırmacıların (özellikle daha genç disiplinlerde) sınırlı teorik yönlendirmeyle çalışmak zorunda olduklarını hatırlatıyor. Böyle bir durumda Üç Kuralını (A Rule of Three) öneriyor:\n\nA Rule of Three (ART): A statistical specification with more than three explanatory variables is meaningless (Achen 2002, 446).\n\nYani Achen’a göre bir sürü kontrol değişkeniyle, karmaşık modeller hesaplamak yerine üç bağımsız değişkenle daha basit bir analiz yapmak, mesela çapraz tablolar kullanmak, çok daha aydınlatıcı olabilir. Hatta Achen bir adım daha ileri gidip örneklemi bölmeyi ve her alt-örneklem üzerinden ayrı ayrı analiz yapmayı öneriyor.3 Böylece gruplar arasındaki farkların (farklı hikayelerin) daha iyi yakalanabileceğini iddia ediyor.\nKısaca özetlemek gerekirse, Achen:\nTeorilerin ve bunlardan çıkan formel modellerin istatiksel analizi yönlendirmesi gerektiğini,\nBöyle bir formel model yoksa, analizin çok daha titiz ve tutumlu yapılması, bir sürü değişkenin analize rastgele atılmaması gerektiğini söylüyor ve üç kuralını öneriyor.\nİlk maddeye en azından kağıt üzerinde çoğu araştırmacının itiraz edeceğini sanmıyorum. Elbette böyle bir uygulama gerçekçi değil. Veri analizi pratikte çok daha karmaşık, kitaplarda sırayla takip edilmesi önerilen adımların içiçe geçtiği bir süreç. Bilimsel makalelerde, sanki öyle değilmişçesine, her şey kitabına uygun yapılmış gibi sunulması durumu değiştirmiyor.\nİkinci maddeye de bir yere kadar itiraz edileceğini düşünmüyorum, yani üç kuralına kadar. Öncelikle üç değişken tercihini keyfi bulanlar olacaktır. Achen da neden üçü seçtiğine dair pek ikna edici bir açıklama sunmuyor (deneyim; iki değişkenin az, dört değişkenin fazla olması?). Ancak kanımca Achen’ın derdi de insanları ikna etmek değil, dikkat çekmek. Makaleyi zaman zaman alaycı bir dille yazması da bunun göstergesi.4 Ayrıca örneklemi bölme fikri (başka bir tabirle stratification) herkes tarafından kabul görmeyecektir. Mesela böyle bir durumda (stratified) parametre tahminleri arasındaki farkı istatiksel olarak test etme imkanı ortadan kalkıyor.5\nYukarıda bahsettiğim muğlaklık dışında iki muhtemel sorun daha var: (1) üç kuralının farklı disiplinlerdeki karşılığı ve (2) uygulamada keyfi olarak yorumlanması. Achen metodoloji tartışmasını, başlıktan da anlaşılacağı üzere, siyaset bilimi özelinde yürütüyor. Mesela analiz biriminin ülkeler olduğu bir araştırmada, birden fazla ekonomik ya da idari gösterge kullanmak (örn. World Bank governance indicators) multicollinearity sorununa yol açabilir. Kural böyle bir durumda işlevsel olabilir. Ancak analiz biriminin bireyler olduğu bir araştırmada, sosyodemografik ya da sosyoekonomik kontrolleri kullanmamak (Achen’ın verdiği örneklerin aksine) eleştirilebilir. Mesela sosyoloji ve psikoloji araştırmalarında üç kuralına uymak adına önemli bir sosyodemografik faktörü dışarda bırakmak göze batacaktır. Tam bu noktada sorun, Achen’ın tasvir ettiği gibi formel modelle ilgili keskin bir ayrımın (var/yok) olmaması.\nDiğer mesele ise uygulamada keyfi yorumlama ihtimali. Mesela formel model olmamasına rağmen, literatürde önemli olduğu gösterilen bir faktörü üç kuralına uymak adına dışarıda bırakmak. Bir grup değişkenden neden üçünün seçildiğini, diğerlerinin dışarıda bırakıldığını ya da bir verinin neden belli bir şekilde bölündüğünü açıklamamak vs. Achen’ı takip edip bu keyfi yaklaşımı gösteren makaleler olup olmadığına ayrıca bakmak gerekiyor. Dolayısıyla elimde bir örnek olmadığını, bunu sadece bir risk olarak gördüğümü söylemeliyim.\nKısacası Achen’ın mesajının ruhunu geçerli bulsam da bunun kendini gösterdiği biçim (ART) tartışmalı gözüküyor. Kanımca daha genel olarak alınacak ders, veri analizinde Achen’ın gösterilmesini istediği titizliğin bu tür kuralları birebir uygulamayı zorlaştırması.\n\n\n\nAchen, Christopher H. 2002. “Toward A New Political Methodology: Microfoundations and ART.” Annual Review of Political Science 5 (1): 423–50.\n\n\nAslında düşününce Achen’ın üç kuralı demekte de bir sıkıntı yok.↩︎\nKimi zaman iki kategorili bağımlı değişkeni örtük (latent) bir değişken olarak modellemek de denir (örn., \\(y^*=X_i\\beta + u\\)). Burada Achen için önemli mesele gözlemlenmeyen stokastik değişken (hata (error) da denir) olan \\(u\\)’nun kümülatif dağılım fonksiyonunun simetrik olması. Böyle simetrik bir dağılımın yarattığı kısıtlamaları aşmak için scobit ya da power logit kullanılmasını tartışıyor (Achen 2002, 427–28).↩︎\n“If one needs several more controls, then there is too much going on in the sample for reliable inference. No one statistical specification can cope with the religious diversity of the American people with respect to abortion attitudes, for example. We have all done estimations like these, underestimating American differences and damaging our inferences by throwing everyone into one specification and using dummy variables for race and denomination. It’s easy, but it’s useless, and we need to stop.” (Achen 2002, 446)↩︎\n“At this point, no doubt, empirical investigators and methodologists accustomed to contemporary political science norms will object. “Look,” they will say, “this new Glockenspiel estimator may not have those frou-frou microfoundations you insist on, but it makes theoretical sense by my lights: It takes account of the yodeled nature of my dependent variable, which ordinary regression ignores. Plus it can be derived rigorously from the Cuckoo distribution. Besides, it fits better. The graphs are pretty, at least if not looked at too closely, and the likelihood ratio test rejects the ordinary regression fit at the 0.05 level. Theory-schmeary. Our job is to let the data decide. I’m going to use Glockenspiel. Anything else is choosing a poorer fit.” Nearly all of us methodologists have shared these views at some stage of our professional lives.\" (Achen 2002, 440)↩︎\nAchen böyle bir eleştiriye araştırma amaçlarının farklı olabileceği (mesela karşılaştırma olmadığı) cevabını verebilir.↩︎\n",
    "preview": {},
    "last_modified": "2021-02-10T14:32:26+03:00",
    "input_file": {}
  },
  {
    "path": "posts/whystat/",
    "title": "İstatistik, Sosyal Bilimler ve Diğer Meseleler: Giriş",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2020-06-06",
    "categories": [],
    "contents": "\nDisclaimer: All my blog posts up to this point were in English. But I’ve decided to write also in Turkish. Scholars should discuss the use of statistical methods more often in Turkey. So using this blog, I would like to take part in these discussions on quantitative methods and statistical analysis in the social sciences.\nKısa bir zaman önce bu blogta Türkçe yazılar yazmaya karar verdim. Bunun arkasındaki en önemli sebep insanlarla tartışırken gönderme yapabileceğim ve gerektiğinde gösterebileceğim, fikirlerimi derli toplu tuttuğum bir yer olması.\nÜzerine yazdığım konularda bir uzmanlık iddiam yok. Sosyal bilimciyim (sosyolog) ve coğu sosyal bilimci gibi istatistik yöntemlerini araştırma için öğrendim. Ancak zaman içinde ilgim uygulama kadar istatistiğin temel kavramlarına ve bilimsel pratik içindeki yerine kaydı. Yani sosyal bilimcilerin alet çantasındaki herhangi bir araç olarak değil, belirsizlik üzerine düşünmenin sistematik yolu olarak istatistik daha fazla ilgimi çekmeye başladı. Bu kısa açıklamadan sonra aşağıda hangi konular üzerinde durmayı planladığımı ve nedenlerini bulabilirsiniz:\nSosyal Bilimlerde İstatistik: Sosyal bilimlerin şüpheli ve yeterince bilimsel olmadığı görüşü sık sık dile getirilir. Bu görüşlerin dile getirilmesinin arkasındaki sebepler (ve geçerlilikleri) bir tarafa, çeşitli nedenlerle (örn. araştırma nesnesi) sosyal bilimlerin belirsizliği daha fazla kucakladığı iddia edilebilir. Tam da bu sebeple belirsizliğin bilimi olarak istatistik (ve olasılık teorisi) bize yardımcı olabilir. Yazdıklarımın genel olarak sosyal bilimcilerin istatistikle ilgili fikirlerine ters düştüğünün farkındayım: Kimi sosyal bilimciler istatistiksel yöntemler kullandıklarında araştırmanın daha nesnel, bilimsel ve kesin hale geldiğini düşünebilir. Ancak kanımca bu yanıltıcı. İstatistik kesinlik kazandırmak bir tarafa, bize araştırmanın sınırlarını gösterebilir ve bizi ne tür çıkarımlar yapılabileceği (çoğu zaman yapılamayacağı) konusunda uyarabilir. Mesela araştırmacı hata (error) kaynaklarını, yanlılık (bias) sorunlarını ve bunların veri analizine olası etkilerini dikkate alarak karar verme imkanına kavuşur. Bu durumun bizi araştırma sonuçlarını yorumlarken daha temkinli olmaya iteceğini düşünüyorum.1\nAncak bu yazdıklarımın pratikte fazla geçerli olmadığının farkındayım. İstatistik yöntemlerini ritüel gibi kullanmak sosyal bilimlerde (ve diğer bilimlerde) yaygın. Bunun bazı sebeplerini başka bir yazıda tartıştım. Ancak yaygınlığı ve benim de bu hatalara zaman zaman düşmem, konuyu tekrar ele almayı gerekli kılıyor. Dolayısıyla genel bir tema olarak, sonraki yazılarda sosyal bilimlerde istatistik konusu üzerinde duracağım.\nİstatistiksel Çıkarım, Hipotez Testi, p-değeri: Üzerinde durmayı planladığım konulardan biri istatistiksel çıkarım, özellikle p-değerine dayalı çıkarımlar. Sıfır Hipotezi Anlamlılık Testi (Null Hypothesis Significance Testing) ve p-değerinin sorunları uzun bir süredir tartışılıyor. Bu sorunların üstesinden gelmek için çeşitli öneriler mevcut ancak Türkiye’de sosyal bilimcilerin konu üzerinde yeterince durduğunu düşünmüyorum. Hem yapılan eleştirileri hem de mevcut pratik açısından öngörülen değişiklikleri tartışmayı planlıyorum.\nBayesyen İstatistik: Yukarıdaki (klasik) yaklaşıma bir alternatif olarak Bayesyen istatistik son zamanlarda sosyal bilimciler arasında daha bilinir hale geldi. Ancak işleri oldukça kolaylaştıran yazılımlara rağmen bu bilinirliğin uygulamaya yansıdığını düşünmüyorum. Bunun önemli bir sebebi Bayesyen istatistiğin daha fazla olasılık bilgisi ve kullanılan yöntemlere dair daha derin bir kavrayış gerektirmesi. Yani sık kullanılan istatistik testlerinin perde arkasında neler yaptığını bilmek gerekiyor. Literatürde oldukça başarılı giriş kitapları mevcut. Benim amacım da Bayesyen istatistiği yeni öğrenen biri olarak deneyimlerimi paylaşmak.\nModelleme:\n\nModel nedir? Model Avusturya tren tarifesi gibidir. Avusturya’da trenler her zaman geç gelir. Prusyalı bir ziyaretçi Avusturyalı bir kondüktöre neden tarifeyi basmaya zahmet ettiklerini sorar. Kondüktör, “Eğer basmazsak trenin ne kadar geç kaldığını nasıl bileceğiz?” diye cevap verir.\n\nYaptığımız en sıradan analizleri bile bir tür modelleme olarak düşünebiliriz. Mesela bir değişkenin ortalamasını almak yani değişken içindeki farklı değerleri tek bir sayıyla özetlemek (yani bir anlamda indirgemek) basit bir model kullanmaktır. Ancak McElreath’ın tabiriyle “küçük dünyalar” olarak modeller dışarıdaki büyük dünyanın eksik bir temsilidir; onun karmaşıklığını tam olarak yakalayamazlar.2 Ancak bize “ne kadar geç kaldığımızı” söyleyebilirler.3 Dolayısıyla bir modelin inşasına (ne, nasıl, hangi amaçla test ediliyor?), varsayımlarına ve performansına bakmak bize yüzeysel bir yorumun ötesine geçme imkanı verecektir. Bu konu üzerinde durmanın kendi araştırma pratiğimi daha fazla irdelememe de yardımcı olacağını düşünüyorum.\nKonuların hepsi oldukça geniş ve başta da belirttiğim gibi bir uzmanlık iddiam yok. Buna rağmen en azından bir tartışma imkânı doğurması için bu konular üzerine yazmak istiyorum. Ayrıca yazıların bir kişisel web sitesinde yayınlandığını hatırlatmalıyım. Belli bir akademik standardı tutturmak istesem de kendimi çok da zorlamayacağım. Bu sınırlılıklara rağmen umarım okuyana keyif verir. Zor olduğu ve bence kötü gözüktüğü için siteye bir forum eklentisi kurmadım. Ancak bana sosyal medya ya da e-posta yoluyla ulaşabilirsiniz.\n\n\nJones, Kelvyn. 2010. “The practice of quantitative methods.” In Theory and Methods in Social Research, edited by Bridget Somekh and Cathy Lewin, 201–11. Sage.\n\n\nBurada istatiksel yöntemlerle veri analizini pozitivizme eşitleyen yaklaşımı yanlış bulduğumu belirtmeliyim. Bu konuda bir tartışma için bknz. (Jones 2010).↩︎\nJohn Ball bu durumun bir avantaj olduğunu çünkü asla işsiz kalmayacağımızı söyler.↩︎\nG.E.P. Box’un sık sık (hatta fazla sık) alıntılanan sözüyle “bütün modeller yanlıştır ancak bazıları kullanışlıdır.”\n\n↩︎\n",
    "preview": {},
    "last_modified": "2021-08-15T15:34:33+03:00",
    "input_file": {}
  },
  {
    "path": "posts/nonp/",
    "title": "Nonparametric Tests and Why I am Wary of Using Them",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2020-05-03",
    "categories": [],
    "contents": "\nIf you are following online communities where people ask for statistics advice, you’ve probably seen a type of question popping up from time to time: “I am doing an analysis using parametric test X, however, (assumption check) test A reveals that assumption B is violated. What should I do?” (As you may guess, the violated assumption is usually the normality assumption). And people start recommending a number of textbook solutions. One such solution is to stop using the parametric test and to prefer a nonparametric equivalent. For example, if you use \\(t\\) test and violate the “normality” assumption, some would say that you should use Mann-Whitney U test. And that’s it.\nI think these questions are problematic for two reasons. First, they are asking for recipe-like solutions, which could be justified by a few references to the extant literature, without thinking about the specific issues related to the analysis at hand. Second, they encourage binary thinking: “If X is violated, do Y, if not do Z.” It would be better to think about the possible consequences of such violations and act accordingly. Otherwise you might be choosing between two faulty approaches.\nI am not against using nonparametric tests per se (they might even have desirable properties), but the way decisions are made regarding these tests.  In my case, the foremost reason why I hesitate using nonparametric tests is that they do not generalize well to complex models. It is not an issue to use a nonparametric test comparing two groups. But what happens if you want to estimate cross-level interactions and random effects? As this answer in Cross Validated indicates, we can think parametric assumptions as simplifying heuristics:\n\nMore generally, parametric procedures provide a way of imposing structure on otherwise unstructured problems. This is very useful and can be viewed as a kind of simplifying heuristic rather than a belief that the model is literally true. Take for instance the problem of predicting a continuous response \\(y\\) based on a vector of predictors \\(x\\) using some regression function \\(f\\) (even assuming that such a function exists is a kind of parametric restriction). If we assume absolutely nothing about \\(f\\) then it’s not at all clear how we might proceed in estimating this function. The set of possible answers that we need to search is just too large. But if we restrict the space of possible answers to (for instance) the set of linear functions \\(f(x)=\\sum_{j=1}^p \\beta_j x_j\\) then we can actually start making progress. We don’t need to believe that the model holds exactly, we are just making an approximation due to the need to arrive at some answer, however imperfect.\n\nThus, such a simplifying heuristic (e.g., specifiying a regression function) would enable one to estimate the parameters of a complex model.1\n\n\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies. Springer International Publishing.\n\n\nIn his Regression Modeling Strategies (2015, 359), Harrell argues that semiparametric models (e.g., proportional odds logistic regression) have many advantages such as “robustness and freedom from all distributional assumptions for Y conditional on any given set of predictors.” I would prefer such an approach if needed.↩︎\n",
    "preview": {},
    "last_modified": "2021-02-10T14:38:00+03:00",
    "input_file": {}
  },
  {
    "path": "posts/gb/",
    "title": "Poor Man's Galton Board",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2019-03-31",
    "categories": [],
    "contents": "\nSome of you might have seen a device called the “Galton Board” (also called the bean machine or quincunx) on social media, or more correctly, its desktop version by Four Pines Publishing. It got popular for a brief moment several months ago. Even Michael from Vsauce posted a video on it:\n\n\nThe device demonstrates central limit theorem, specifically how binomial distribution approximates to normal distribution. As you can see in the video, there are pegs on the board arranged in a triangular shape. You drop a single bean, the bean hits the peg and falls left or right with some probability (\\(p\\)). Since we assume that the device is constructed well (i.e., unbiased), we expect the bean goes both sides with equal probability, \\(p=1-p=q=0.5\\). This step is repeated for each row of pegs and the bean ends up in a (corresponding, rectangular) bin. If the probability of bouncing right is \\(p\\) (in our case, \\(0.5\\)), the number of rows is \\(N\\), and the number of times the bean bounces to right is \\(n\\), then the probability of the bean ending up in the \\(n\\)th bin from left is,\n\\[\\left( \\begin{array}{c} N \\\\ n \\end{array}\\right)=p^nq^{N-n},\\]\nwhich is probability mass function of a binomial distribution. Here is the catch: according to de Moivre-Laplace theorem (a special case of CLT), under certain conditions, this binomial distribution will approximate to the probability density function of a normal distribution with mean, \\(np\\) and variance \\(npq\\). In this case, if the number of rows (of pegs) and beans are large enough, the distribution would approximate to normal distribution, as the small Galton board (with 3000 beads and 12(?) rows of pegs) demonstrates.\nI really like this kind of small devices, but I am not willing to pay $39.95 (on Amazon). And, although the pleasure of watching the beans is missing, I can see the approximation at work using R:\n\n\nlibrary(tidyverse)\nlibrary(hrbrthemes)\nset.seed(12)\ndf <- rbinom(3000, 12, 0.5)\n\ndf %>% \n  data.frame() %>% \n  ggplot(aes(.)) + \n  geom_histogram(aes(., stat(density)), binwidth = 1, color=\"white\") +\n  stat_function(fun=dnorm, color=\"black\", args=list(mean=mean(df), sd=sd(df))) +\n  scale_y_continuous(limits=c(0, 0.25), breaks = seq(0, 0.25, 0.05)) +\n  labs(title=\"Poor Man's Galton Board\") +\n  theme_ipsum_rc()\n\n\n\n\nMoreover, I can change the probability of bouncing to left or right, number of beans, and number of pegs (hence, bins) to see whether approximation works or not. (I also overlay a normal curve on histograms using sample mean and standard deviation.)\nTilting the Board\nIt is not hard to guess what would happen if I tilt the board to one side or the other. This will increase the probability of bouncing to left (or right) and we will end up with a skewed distribution.\n\n\n\nDecreasing the Number of Beans\nWhat would happen if I decrease the number of beans? On the left corner, we have the original board with 3000 beans and 12 pegs. Keeping the number of pegs constant, I decrease the number of beans to 1000, 500, and 100. I would say that the distribution of 1000 beans approximate the normal distribution quite well. But it is not the case for the distributions of 500 and 100 beans. One can see some skew, especially in the case of 100 beans.\n\n\n\nIncreasing the Number of pegs\nAnd if I increase the number of pegs (hence, the number of bins), the beans will spread more and more, and the distributions become platykurtic (see the change on x axis labels).\n\n\n\nThere is no way for us to know where a single bean would end up. But under certain conditions, it is possible to know the distribution of thousands of beans. This is what Galton1 (1889) called “Order in Apparent Chaos” (p.66):2\n\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “Law of Frequency of Error.” The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.\n\nUpdate 12 Dec, 2020:\nThere is a beautiful visualization of Galton Board on mikefc’s chipmunkcore repo:\n\n\n{\"x\":{\"twid\":\"1296417067814522881\",\"pars\":null},\"evals\":[],\"jsHooks\":[]}\n\n\n\nGalton, Francis. 1889. Natural Inheritance. MacMillan.\n\n\nPedersen, Thomas Lin. 2017. Patchwork: The Composer of Ggplots. https://github.com/thomasp85/patchwork.\n\n\nRudis, Bob. 2019. Hrbrthemes: Additional Themes, Theme Components and Utilities for ’Ggplot2’. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nWickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\n\nAlthough he is an important figure in the history of statistics, nowadays Galton is criticized for his eugenics and “scientific racism.”↩︎\nNatural Inheritance is available here as PDF.↩︎\n",
    "preview": "posts/gb/gb_files/figure-html5/mp-1.png",
    "last_modified": "2021-02-10T14:34:10+03:00",
    "input_file": {}
  },
  {
    "path": "posts/socapp/",
    "title": "Sorcerer's Apprentice Approach to Statistical Analysis",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\nNumbers do not produce value. The widespread availability of muscular number-crunching computers has had the untoward effect of yielding power to the sorcerer’s apprentice. Statistics and computers must support the research activity, not motivate it (Alfred 1987, 3).\n\nAlfred criticized neglecting the role of theory (which should guide any reseach inquiry) for the sake of “number-crunching.” This is a fair argument and repeated frequently by others. I would like to talk about a different (but related) issue using the metaphor, sorcerer’s apprentice. I am less concerned with the role of theory and its relation to method, but I am interested in how statistical methods are used in practice. But first, what does sorcerer’s apprentice mean?\nIt is the name of a poem by Goethe (Der Zauberlehrling).1 The story is about a sorcerer’s apprentice who is left alone to do some chores:\n\nTired of fetching water by pail, the apprentice enchants a broom to do the work for him – using magic in which he is not yet fully trained. The floor is soon awash with water, and the apprentice realizes that he cannot stop the broom because he does not know how. The apprentice splits the broom in two with an axe – but each of the pieces becomes a whole new broom that takes up a pail and continues fetching water, now at twice the speed.\n\nIn the end, the master returns and saves the apprentice.2 The moral of the story: one should not dare to do things beyond his/her understanding. In our version, apprentice is an academic,3 and the broom is “statistics and computers.” Yet, I think computers (or rather the software we use for analysis) could become “broom” alone. And it might lead to a lack of understanding of statistics. In fact, this is different from neglecting the role of theory and might happen even if someone has a strong theoretical background and guidance.\n\n\nknitr::include_graphics(\"sa.png\")\n\n\n\n\nFigure 1: Der Zauberlehrling by Ferdinand Barth (1842–1892)\n\n\n\n\nThere are various reasons why this could happen, especially in the social sciences.4 I would like to discuss three of them. The first and well-known reason (already stated by Alfred) is the availability of computers and increasing (computational) power of statistical software. Nowadays, we have easy access to the state-of-art programs which implement complex statistical techniques. The information on where to click or few lines of code are usually available online. The software returns an output and all you need to know is what those numbers in the output mean (I might be exaggerating). But the software availability alone is not enough and this takes us to the second (and more important) reason: the golems of the analysis.\nIn his book, McElreath (2015) begins the first chapter with the example of statistical golems which refer to the “tests” widely used in statistical analyses and taught in introductory courses. Those of us who took these courses are familiar with these golems, for example:\n\nWhenever someone deploys even a simple statistical procedure, like a classical t-test, she is deploying a small golem that will obediently carry out an exact calculation, performing it the same way (nearly) every time, without complaint. (McElreath 2015, 2)\n\nIt is easy to find compilations of these golems, and guidelines on how to choose among them. A quick google search with “which statistical test to use” would return dozens of flowcharts (McElreath provided one in his book).\nIt is neither possible to deny the usefulness of these golems nor practical to get rid of them completely as they still might have some pedagogical value. But we need to keep in mind McElreath’s warning that “…there is no wisdom in the golem. It doesn’t discern when the context is inappropriate for its answers. It just knows its own procedure, nothing else” (McElreath 2015, 2).5 So, it is not simply a matter of computational power and software implementation, but rather, how we use these two together. Unfortunately, the “know-how” about these golems might conceal the ignorance of the actual analytical techniques.\nThese arguments seem to imply that the burden is on the shoulders of the researcher. To a certain extent, this is true: the researcher should act responsibly, and explain the procedures and rationale behind them. This requires giving some thought to the tests instead of treating them as ready-made solutions (golems). But there is a third reason as to why this is not always possible. And it is time and resource constraints.\nIt is expected that a graduate student should master their substantive research area and the statistical methods they use (or in general, research methods). But is this really possible? If so, to what extent? If a student does not come to graduate school with a strong background in statistics, most graduate courses/seminars would not take far beyond an intermediate level. For basic applications, this could be enough. But in practice, we have to deal with intricate problems which require expertise.6\nUsually, we need to prioritize what to study, especially under the pressure to finish projects and publish. Learning and mastering statistical methods can be time consuming and less rewarding. There is a trade-off and the decision is not that difficult: first and foremost, we are expected to contribute to our substantive area of research. There are lots of golems out there doing the job. And we saw others (senior academics?) using them without much hesitation. So lurks the danger of becoming a sorcerer’s apprentice.\nHere is my two cents on how to avoid this approach as much as possible:\n\nWe might not be able to master statistical methods, their mathematical representation, or their software implementation, but it is possible to learn, at least in simpler terms, what is going on behind the curtain. For instance, there are many accessible discussions on the assumptions of statistical techniques and what could happen in case of violation.\nClose attention to model specification would help as well. For example, one should be clear about the rationale of including some variables, while excluding others. It would be misleading to dump all variables, and then eliminating them one by one in an arbitrary fashion (e.g., p-value fishing). If we are not careful enough about the model specification, we will end up with “garbage in, garbage out” models.\nAfter running the model, it is good to keep your guard up and remain suspicious about the results, instead of accepting them as presented in the computer output. It is possible that you missed some important issue at some point in the previous steps of the analysis. Making sensible changes (but not fishing for significant results or larger effect sizes), and testing for sensitivity would be helpful to improve the quality of the analysis.\n\nNote: A friend commented that these arguments mostly apply to secondary data analysis. For example, in experimental research, there are other issues that require more attention (such as randomization, power, treatment, etc.), but they might not encounter the above mentioned problems to the same extent (e.g., variable selection, model specification). I agree with my friend, since I had secondary data analysis in my mind while writing this post. But I also think that the experimental researchers are no less susceptible to become a sorcerer’s apprentice. However, they might need to adopt different precautionary measures.\n\n\n\n\nAlfred, Braxton M. 1987. Elements of Statistics for the Life and Social Sciences. London Paris Tokyo: Springer-Verlag.\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103: 677–80.\n\n\nI think the German word captures the diminutive sense better.↩︎\nYou might not know that this is Goethe’s poem, but maybe the story sounds familiar. The reason is that you can find references to the Sorcerer’s Apprentice from the Communist Manifesto to Disney’s Fantasia (1940).↩︎\nIt might mean faculty member or graduate student. Honestly, I don’t think it matters a lot here.↩︎\nI think social sciences are particularly susceptible, but this could be an overstatement.↩︎\nThe issue is not limited to this test and similar others. For example, in the introductory courses, we learn four scales of measurement (Stevens 1946): nominal, ordinal, interval, and ratio. Now imagine we have a variable showing number of children in a household and response categories are “0, 1, 2, 3, 4, 5+.” How would you classify such a variable based on the four scales above? It is not nominal, not interval or ratio (e.g., 5+). One can say ordinal here, but we might lose information by treating it ordinal, especially for the first response categories. A better category would be (right-) censored count variable which can be analyzed (as an outcome) using a censored poisson regression model (Cameron and Trivedi 2005).↩︎\nOne can argue that that is why we have academic advisors. But again, unless their substantive area of research includes statistical methods, can we really trust them? Are they more careful in using these statistical golems? I would prefer to err on the side of caution. So, unlike the sorcerer’s apprentice, we might not have a master to save the day.↩︎\n",
    "preview": "posts/socapp/sa.png",
    "last_modified": "2021-02-10T14:33:27+03:00",
    "input_file": {}
  },
  {
    "path": "posts/climbing/",
    "title": "Maximum Likelihood Estimation: Finding the Top of a Hill",
    "description": {},
    "author": [
      {
        "name": "Tahir Enes Gedik",
        "url": {}
      }
    ],
    "date": "2018-02-06",
    "categories": [],
    "contents": "\nI think one of the most intuitive descriptions of the maximum likelihood estimation (especially for the beginners) can be found in Long and Freese (2014):\n\nFor all but the simplest models, the only way to find the maximum likelihood function is by numerical methods.1 Numerical methods are the mathematical equivalent of how you would find the top of a hill if you were blindfolded and knew only the slope of the hill at the spot where you are standing and how the slope at that spot is changing which you could figure out by poking your foot in each direction. The search begins with start values corresponding to your location as you start your climb. From the start position, the slope of the likelihood function and the rate of change in the slope determine the next guess for the parameters. The process continues to iterate until the maximum of the likelihood function is found, called, convergence, and the resulting estimates are reported (Long and Freese 2014, 84)\n\nExample: Logistic Regression\nData preparation\n\n\nlibrary(tidyverse)\nlibrary(optimx) # or optim depending on the optimization method used, \n                # BFGS is available in both packages\ndf <- carData::Mroz\n\noutcome <- fct_recode(df$lfp,\n               \"0\" = \"no\",\n               \"1\" = \"yes\")\noutcome <- as.numeric(as.character(outcome))\n\npredictors <- df %>% \n  select(k5, age, inc) %>%  # selected predictors\n  mutate(int=rep(1, nrow(df))) %>% # column of 1s (intercept)\n  select(int, everything()) %>% \n  as.matrix()\n\n\n\n“The search begins with start values corresponding to your location as you start your climb.”\n\n\n# Use OLS model coefficients as starting values\nlmfit <- lm(outcome ~ predictors[,c(2:4)])\ns_val <- lmfit$coefficients\n\n\n\n“From the start position, the slope of the likelihood function and the rate of change in the slope determine the next guess for the parameters.”\n\n\nlogLikelihood <- function(vBeta, mX, vY) {\n  return(-sum(vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))\n    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))))  \n}\n\n\n\n“The process continues to iterate until the maximum of the likelihood function is found, called, convergence,…”\n\n\noptimization <- optimx(s_val, logLikelihood, method = 'BFGS', \n                       mX = predictors, vY = outcome, hessian=TRUE)\n\n\n\n“…and the resulting estimates are reported.”\n\n\nestimation_optx <- optimization %>%\n  select(1:ncol(predictors)) %>% t()\nestimation_optx\n\n\n                               BFGS\nX.Intercept.             3.39332484\npredictors...c.2.4..k5  -1.31311634\npredictors...c.2.4..age -0.05682900\npredictors...c.2.4..inc -0.01875491\n\nCompare them with the result of glm function:\n\n\nsummary(glm(lfp ~ k5 + age + inc, df, family = binomial))\n\n\n\nCall:\nglm(formula = lfp ~ k5 + age + inc, family = binomial, data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.867  -1.184   0.731   1.003   1.970  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.394398   0.515576   6.584 4.59e-11 ***\nk5          -1.313316   0.187535  -7.003 2.50e-12 ***\nage         -0.056855   0.010991  -5.173 2.31e-07 ***\ninc         -0.018751   0.006889  -2.722  0.00649 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  956.75  on 749  degrees of freedom\nAIC: 964.75\n\nNumber of Fisher Scoring iterations: 4\n\nHere is the wiki page for the BFGS (Broyden–Fletcher–Goldfarb–Shanno algorithm) method, which “belongs to quasi-Newton methods, a class of hill-climbing optimization techniques….”\n\n\n\nLong, Scott J., and Jeremy Freese. 2014. Regression Models for Categorical Dependent Variables Using Stata. Texas: Stata Press.\n\n\nFor a quick explanation of the difference between analytical and numerical methods: What’s the difference between analytical and numerical approaches to problems?↩︎\n",
    "preview": {},
    "last_modified": "2021-02-10T14:32:58+03:00",
    "input_file": {}
  }
]
